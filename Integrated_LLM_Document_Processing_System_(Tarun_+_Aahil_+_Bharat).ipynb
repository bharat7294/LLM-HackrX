{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Full LLM Document Processing System (Integrated Tarun, Aahil, and Bharat's tasks)\n",
        "# This script performs document extraction, cleaning, chunking, embedding,\n",
        "# FAISS indexing, and uses a generative LLM for structured decision-making.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import requests\n",
        "import io\n",
        "import glob\n",
        "import email # For EML parsing\n",
        "\n",
        "# --- Unified Library Installation and Imports ---\n",
        "print(\"--- Initializing: Installing necessary libraries ---\")\n",
        "try:\n",
        "    # Use !{sys.executable} -m pip to ensure installation into the correct Python environment\n",
        "    !{sys.executable} -m pip install requests python-docx PyMuPDF faiss-cpu transformers sentence-transformers -qq --upgrade\n",
        "\n",
        "    # Import all libraries after ensuring they are installed\n",
        "    import fitz # PyMuPDF\n",
        "    import docx # python-docx\n",
        "    from urllib.parse import urlparse\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    import faiss\n",
        "    import torch\n",
        "    from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, T5ForConditionalGeneration\n",
        "\n",
        "    print(\"‚úÖ All required libraries checked/installed successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error during library installation: {e}\")\n",
        "    print(\"Please ensure your Colab environment has internet access and try again.\")\n",
        "    sys.exit(1) # Exit the script if critical libraries cannot be installed\n",
        "\n",
        "print(\"\\n--- Starting Document Processing Pipeline ---\")\n",
        "\n",
        "# --- 1. Tarun's Task: Data Extraction and Cleaning ---\n",
        "print(\"\\n--- Tarun's Task: Data Extraction and Cleaning ---\")\n",
        "\n",
        "def download_file(url: str) -> bytes:\n",
        "    \"\"\"Downloads content from a given URL.\"\"\"\n",
        "    print(f\"Attempting to download from URL: {url}\")\n",
        "    try:\n",
        "        r = requests.get(url, timeout=30)\n",
        "        r.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "        return r.content\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading {url}: {e}\")\n",
        "        return b\"\"\n",
        "\n",
        "def extract_text_from_pdf_bytes(bts: bytes) -> str:\n",
        "    \"\"\"Extracts text from PDF bytes.\"\"\"\n",
        "    text = []\n",
        "    try:\n",
        "        with fitz.open(stream=bts, filetype=\"pdf\") as doc:\n",
        "            for page in doc:\n",
        "                text.append(page.get_text())\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF bytes: {e}\")\n",
        "        return \"\"\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "def extract_text_from_docx_bytes(bts: bytes) -> str:\n",
        "    \"\"\"Extracts text from DOCX bytes.\"\"\"\n",
        "    bio = io.BytesIO(bts)\n",
        "    text = []\n",
        "    try:\n",
        "        doc = docx.Document(bio)\n",
        "        paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]\n",
        "        text = paragraphs\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from DOCX bytes: {e}\")\n",
        "        return \"\"\n",
        "    return \"\\n\".join(text)\n",
        "\n",
        "def extract_text_from_eml_bytes(bts: bytes) -> str:\n",
        "    \"\"\"Extracts text from EML bytes (email files).\"\"\"\n",
        "    msg = email.message_from_bytes(bts)\n",
        "    parts = []\n",
        "    for part in msg.walk():\n",
        "        content_type = part.get_content_type()\n",
        "        if content_type == \"text/plain\":\n",
        "            payload = part.get_payload(decode=True)\n",
        "            try:\n",
        "                parts.append(payload.decode(part.get_content_charset() or 'utf-8', errors=\"ignore\"))\n",
        "            except:\n",
        "                parts.append(payload.decode(errors=\"ignore\"))\n",
        "        elif content_type == \"text/html\":\n",
        "            payload = part.get_payload(decode=True)\n",
        "            try:\n",
        "                parts.append(payload.decode(part.get_content_charset() or 'utf-8', errors=\"ignore\"))\n",
        "            except:\n",
        "                parts.append(payload.decode(errors=\"ignore\"))\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "def extract_text_from_file(path: str) -> str:\n",
        "    \"\"\"Extracts text from a local file (PDF, DOCX, or EML).\"\"\"\n",
        "    print(f\"Extracting text from local file: {path}\")\n",
        "    with open(path, \"rb\") as f:\n",
        "        content = f.read()\n",
        "    if path.lower().endswith(\".pdf\"):\n",
        "        return extract_text_from_pdf_bytes(content)\n",
        "    elif path.lower().endswith(\".docx\"):\n",
        "        return extract_text_from_docx_bytes(content)\n",
        "    elif path.lower().endswith(\".eml\"):\n",
        "        return extract_text_from_eml_bytes(content)\n",
        "    else:\n",
        "        print(f\"Warning: Unsupported file type for local file {path}. Attempting to decode as plain text.\")\n",
        "        return content.decode(errors=\"ignore\")\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Cleans the extracted text by removing excessive newlines/spaces\n",
        "    and retaining logical paragraph breaks.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "    text = re.sub(r'(?<!\\n)\\n(?![\\n\\s])', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Tarun's main execution part\n",
        "TARUN_OUTPUT_DIR = \"cleaned_texts\"\n",
        "os.makedirs(TARUN_OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Cleaned texts (in JSON) will be saved in: {TARUN_OUTPUT_DIR}\")\n",
        "\n",
        "processed_documents = []\n",
        "local_files = glob.glob(\"*.pdf\") + glob.glob(\"*.docx\") + glob.glob(\"*.eml\")\n",
        "\n",
        "if not local_files:\n",
        "    print(\"No local PDF, DOCX, or EML files found. Please upload your documents to Colab.\")\n",
        "    print(\"Example: Drag and drop 'policy.pdf' into the file browser on the left.\")\n",
        "    sys.exit(\"No documents found to process. Please upload files and run again.\")\n",
        "\n",
        "for filepath in local_files:\n",
        "    print(f\"\\n--- Processing {filepath} ---\")\n",
        "    raw_text = \"\"\n",
        "    if filepath.lower().endswith((\".pdf\", \".docx\", \".eml\")):\n",
        "        raw_text = extract_text_from_file(filepath)\n",
        "    else:\n",
        "        print(f\"Skipping {filepath}: Unsupported file type.\")\n",
        "        continue\n",
        "\n",
        "    if raw_text:\n",
        "        extracted_text = clean_text(raw_text)\n",
        "        processed_documents.append({\n",
        "            \"filename\": os.path.basename(filepath),\n",
        "            \"cleaned_text\": extracted_text\n",
        "        })\n",
        "        print(f\"‚úÖ Processed {os.path.basename(filepath)}\")\n",
        "        # Optional: Keep this preview if you still want to see it, otherwise comment out\n",
        "        # print(f\"Preview:\\n{extracted_text[:min(len(extracted_text), 2000)]}{'...' if len(extracted_text) > 2000 else ''}\\n\")\n",
        "    else:\n",
        "        print(f\"Skipping {filepath} due to extraction errors or empty content.\")\n",
        "\n",
        "TARUN_OUTPUT_JSON_FILENAME = os.path.join(TARUN_OUTPUT_DIR, \"all_cleaned_documents.json\")\n",
        "with open(TARUN_OUTPUT_JSON_FILENAME, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(processed_documents, f, indent=4)\n",
        "print(f\"\\nüéâ Tarun's Task Completed: All cleaned texts saved to {TARUN_OUTPUT_JSON_FILENAME} in JSON format.\")\n",
        "\n",
        "# --- 2. Aahil's Task: Text Chunking and Embedding Generation ---\n",
        "print(\"\\n--- Aahil's Task: Text Chunking and Embedding Generation ---\")\n",
        "\n",
        "# Configuration for Aahil's Task\n",
        "AAHIL_OUTPUT_DIR = \"chunks_and_embeddings\"\n",
        "CHUNK_SIZE = 500\n",
        "CHUNK_OVERLAP = 50\n",
        "EMBEDDING_MODEL_NAME = 'all-MiniLM-L6-v2'\n",
        "\n",
        "os.makedirs(AAHIL_OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Chunks and embeddings will be saved in: {AAHIL_OUTPUT_DIR}\")\n",
        "\n",
        "print(f\"Loading sentence-transformer model '{EMBEDDING_MODEL_NAME}'...\")\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "print(\"‚úÖ Embedding model loaded successfully.\")\n",
        "\n",
        "def chunk_text(text: str, filename: str, chunk_size: int, chunk_overlap: int) -> tuple[list[str], list[str]]:\n",
        "    chunks = []\n",
        "    chunk_sources = []\n",
        "    current_pos = 0\n",
        "\n",
        "    while current_pos < len(text):\n",
        "        end_pos = min(current_pos + chunk_size, len(text))\n",
        "        chunk = text[current_pos:end_pos]\n",
        "        chunks.append(chunk)\n",
        "        chunk_sources.append(filename)\n",
        "        current_pos += chunk_size - chunk_overlap\n",
        "        if current_pos < 0: # Ensure no negative index\n",
        "            current_pos = 0\n",
        "    return chunks, chunk_sources\n",
        "\n",
        "all_chunks_list = []\n",
        "all_embeddings_list = []\n",
        "all_source_files_list = []\n",
        "\n",
        "if not os.path.exists(TARUN_OUTPUT_JSON_FILENAME) or not processed_documents:\n",
        "    print(f\"‚ùå Error: No cleaned documents found from Tarun's task at '{TARUN_OUTPUT_JSON_FILENAME}'. Exiting Aahil's task.\")\n",
        "else:\n",
        "    try:\n",
        "        with open(TARUN_OUTPUT_JSON_FILENAME, \"r\", encoding=\"utf-8\") as f:\n",
        "            documents_for_aahil = json.load(f)\n",
        "        print(f\"Loaded {len(documents_for_aahil)} documents for chunking and embedding.\")\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"‚ùå Error decoding JSON for Aahil: {e}. Exiting Aahil's task.\")\n",
        "        documents_for_aahil = []\n",
        "\n",
        "    if documents_for_aahil:\n",
        "        for doc in documents_for_aahil:\n",
        "            filename = doc.get(\"filename\", \"unknown_file\")\n",
        "            content = doc.get(\"cleaned_text\", \"\")\n",
        "\n",
        "            if content.strip():\n",
        "                chunks, chunk_sources = chunk_text(content, filename, CHUNK_SIZE, CHUNK_OVERLAP)\n",
        "                print(f\"Generated {len(chunks)} chunks from '{filename}'\")\n",
        "\n",
        "                if chunks:\n",
        "                    with torch.no_grad():\n",
        "                        embeddings = embedding_model.encode(chunks, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "                    all_chunks_list.extend(chunks)\n",
        "                    all_embeddings_list.append(embeddings.cpu().numpy())\n",
        "                    all_source_files_list.extend(chunk_sources)\n",
        "                else:\n",
        "                    print(f\"No chunks generated for '{filename}'. Skipping embeddings.\")\n",
        "            else:\n",
        "                print(f\"Skipping '{filename}' as its cleaned text content is empty.\")\n",
        "\n",
        "    if all_chunks_list and all_embeddings_list:\n",
        "        final_embeddings = np.vstack(all_embeddings_list)\n",
        "\n",
        "        with open(os.path.join(AAHIL_OUTPUT_DIR, \"all_chunks.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(all_chunks_list, f)\n",
        "        with open(os.path.join(AAHIL_OUTPUT_DIR, \"all_embeddings.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(final_embeddings, f)\n",
        "        with open(os.path.join(AAHIL_OUTPUT_DIR, \"source_files.pkl\"), \"wb\") as f:\n",
        "            pickle.dump(all_source_files_list, f)\n",
        "        print(f\"\\nüéâ Aahil's Task Completed: Chunks, embeddings, and source files ready for Bharat.\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Aahil's Task Failed: No chunks or embeddings were generated. Cannot proceed to Bharat's task.\")\n",
        "        sys.exit(1) # Exit if Aahil's task failed\n",
        "\n",
        "# --- 3. Bharat's Task: LLM Document Processing System ---\n",
        "print(\"\\n--- Bharat's Task: LLM Document Processing System ---\")\n",
        "\n",
        "# Configuration for Bharat's Task\n",
        "LLM_MODEL_NAME = \"google/flan-t5-small\"\n",
        "\n",
        "def load_aahil_data(directory: str):\n",
        "    chunks = None\n",
        "    embeddings = None\n",
        "    source_files = None\n",
        "    try:\n",
        "        with open(os.path.join(directory, \"all_chunks.pkl\"), \"rb\") as f:\n",
        "            chunks = pickle.load(f)\n",
        "        with open(os.path.join(directory, \"all_embeddings.pkl\"), \"rb\") as f:\n",
        "            embeddings = pickle.load(f)\n",
        "        with open(os.path.join(directory, \"source_files.pkl\"), \"rb\") as f:\n",
        "            source_files = pickle.load(f)\n",
        "        print(f\"‚úÖ Successfully loaded {len(chunks)} chunks and embeddings of shape {embeddings.shape}\")\n",
        "        return chunks, embeddings, source_files\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: Missing data file in '{directory}'. Please ensure Aahil's script ran successfully and saved all .pkl files.\")\n",
        "        print(f\"Details: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred while loading Aahil's data: {e}\")\n",
        "    return None, None, None\n",
        "\n",
        "chunks, embeddings, source_files = load_aahil_data(AAHIL_OUTPUT_DIR)\n",
        "\n",
        "if chunks is None or embeddings is None or source_files is None:\n",
        "    print(\"Exiting. Cannot proceed with Bharat's task without all data loaded from Aahil's task.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Initialize FAISS Index\n",
        "embeddings = embeddings.astype('float32')\n",
        "dimension = embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "print(f\"‚úÖ FAISS index created and populated with {index.ntotal} vectors.\")\n",
        "\n",
        "# Load Query Embedding Model (same as Aahil's)\n",
        "print(f\"Loading query embedding model '{EMBEDDING_MODEL_NAME}'...\")\n",
        "query_embedding_model_bharat = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
        "print(\"‚úÖ Query embedding model loaded.\")\n",
        "\n",
        "# Load Generative LLM for Decision and Structured Output\n",
        "print(f\"Loading Generative LLM pipeline: '{LLM_MODEL_NAME}'...\")\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "text_generation_pipeline = None\n",
        "try:\n",
        "    text_generation_pipeline = pipeline(\n",
        "        \"text2text-generation\",\n",
        "        model=LLM_MODEL_NAME,\n",
        "        tokenizer=LLM_MODEL_NAME,\n",
        "        device=device,\n",
        "        max_new_tokens=512,\n",
        "        truncation=True\n",
        "    )\n",
        "    print(f\"‚úÖ Generative LLM pipeline loaded successfully on {'GPU' if device == 0 else 'CPU'}.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading Generative LLM pipeline '{LLM_MODEL_NAME}'. This might be due to network issues or model availability.\")\n",
        "    print(f\"Details: {e}\")\n",
        "    print(\"The system will proceed, but will only return relevant chunks, not generate structured answers.\")\n",
        "    text_generation_pipeline = None\n",
        "\n",
        "# --- LLM Document Processing System Interaction Loop ---\n",
        "print(\"\\n--- LLM Document Processing System Ready! Type 'exit' to quit. ---\")\n",
        "while True:\n",
        "    user_query = input(\"\\nYour Query: \")\n",
        "    if user_query.lower() == 'exit':\n",
        "        print(\"Exiting Document Processing System. Goodbye! üëã\")\n",
        "        break\n",
        "\n",
        "    print(\"\\nSearching for relevant clauses... ü§î\")\n",
        "    query_embedding = query_embedding_model_bharat.encode([user_query])\n",
        "    query_embedding = np.array(query_embedding).astype('float32')\n",
        "\n",
        "    k_retrieve = 15 # Increased k for more context\n",
        "    distances, indices = index.search(query_embedding, k_retrieve)\n",
        "\n",
        "    relevant_chunks = []\n",
        "    relevant_sources_info = []\n",
        "\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        if 0 <= idx < len(chunks):\n",
        "            relevant_chunks.append(chunks[idx])\n",
        "            source_info = f\"Source: {source_files[idx]}\"\n",
        "            relevant_sources_info.append(source_info)\n",
        "        else:\n",
        "            print(f\"Warning: FAISS returned an out-of-bounds index: {idx}\")\n",
        "\n",
        "    if not relevant_chunks:\n",
        "        print(\"No highly relevant clauses found for your query. Please try rephrasing.\")\n",
        "        continue\n",
        "\n",
        "    # --- Print statements for debugging (uncomment if you need to see raw chunks) ---\n",
        "    # print(f\"Found {len(relevant_chunks)} relevant clauses. ‚ú®\")\n",
        "    # print(\"\\n--- Relevant Clauses (Full Content for Debugging) ---\")\n",
        "    # for i, chunk in enumerate(relevant_chunks):\n",
        "    #     print(f\"Clause {i+1} ({relevant_sources_info[i]}):\\n{chunk}\\n\")\n",
        "\n",
        "    if text_generation_pipeline:\n",
        "        context_str = \"\\n\".join([f\"Clause {i+1}: {c}\" for i, c in enumerate(relevant_chunks)])\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert document processing system. Your task is to analyze a user query and provided relevant policy clauses to determine a decision, an amount (if applicable), and provide a justification.\n",
        "\n",
        "        **User Query Details:**\n",
        "        The user query is: \"{user_query}\"\n",
        "        From this query, identify these specific details:\n",
        "        - Age of individual:\n",
        "        - Gender of individual:\n",
        "        - Procedure/Condition:\n",
        "        - Location of procedure:\n",
        "        - Policy duration:\n",
        "\n",
        "        **Relevant Policy Clauses:**\n",
        "        {context_str}\n",
        "\n",
        "        **Instructions:**\n",
        "        1.  First, **extract all the specific facts directly from the 'User Query Details' section**. For example, for \"46-year-old male, knee surgery in Pune, 3-month-old insurance policy\", the facts are: Age 46, Gender Male, Procedure Knee Surgery, Location Pune, Policy Duration 3 months.\n",
        "        2.  **Strictly evaluate these extracted facts against the 'Relevant Policy Clauses'**.\n",
        "        3.  Determine the **\"Decision\"** (e.g., \"Approved\", \"Rejected\", \"Pending Further Review\"). Base this solely on whether the extracted query facts meet the conditions in the clauses.\n",
        "        4.  Determine the **\"Amount\"** if an exact value is specified or can be clearly inferred from the clauses based on the user query's details. If no specific amount is mentioned for the given conditions, state \"N/A\" or \"To be determined\" with a brief reason.\n",
        "        5.  Provide a **\"Justification\"** that clearly explains your decision. You **MUST explicitly reference the clause numbers** (e.g., \"Clause 1 states...\") and quote or paraphrase the specific parts of the clauses that support your decision for each extracted fact. Make sure to link each query detail to the clause that governs it, especially regarding the location mentioned in the query.\n",
        "        6.  Your output MUST be a **well-formed JSON object** with the following keys: \"Decision\", \"Amount\", \"Justification\".\n",
        "\n",
        "        **Example Response Format:**\n",
        "        ```json\n",
        "        {{\n",
        "          \"Decision\": \"Approved\",\n",
        "          \"Amount\": \"N/A\",\n",
        "          \"Justification\": \"Based on Clause 1: 'Cataract surgery is covered for individuals above 50 years of age...' (patient is 55). Clause 1 also states: '...provided the policy has been active for a minimum of 90 days (3 months)...' (policy is 6 months old). Clause 2 states: 'All surgical procedures performed at network hospitals within major metropolitan cities, including Mumbai, are eligible for 100% coverage...' (procedure is in Mumbai).\"\n",
        "        }}\n",
        "        ```\n",
        "        \"\"\"\n",
        "        print(\"\\nGenerating structured response... ü§ñ\")\n",
        "        try:\n",
        "            llm_output = text_generation_pipeline(prompt)[0]['generated_text']\n",
        "\n",
        "            # --- ALWAYS PRINT RAW LLM OUTPUT FOR DEBUGGING THIS ERROR ---\n",
        "            print(\"\\n--- Raw LLM Output (for debugging) ---\")\n",
        "            print(llm_output)\n",
        "\n",
        "            json_str = \"\"\n",
        "            # Try to extract content within ```json ... ``` markdown block first\n",
        "            json_match = re.search(r'```json\\n(.*?)```', llm_output, re.DOTALL)\n",
        "            if json_match:\n",
        "                json_str = json_match.group(1).strip()\n",
        "            else:\n",
        "                # If no markdown block, try to find the JSON object directly\n",
        "                # Find the first '{' and the last '}'\n",
        "                start_idx = llm_output.find('{')\n",
        "                end_idx = llm_output.rfind('}')\n",
        "                if start_idx != -1 and end_idx != -1 and start_idx < end_idx:\n",
        "                    json_str = llm_output[start_idx : end_idx + 1].strip()\n",
        "                else:\n",
        "                    # Fallback to the entire output if JSON delimiters not found\n",
        "                    # This case will likely lead to JSONDecodeError if not pure JSON\n",
        "                    json_str = llm_output.strip()\n",
        "\n",
        "            # Check if extracted json_str is empty before attempting to load\n",
        "            if not json_str:\n",
        "                print(\"‚ö†Ô∏è Warning: Extracted JSON string is empty. LLM might not have generated valid JSON.\")\n",
        "                # You might want to skip json.loads and indicate a failure here\n",
        "                # For now, we proceed to let json.loads raise the error, which gives \"char 0\"\n",
        "                # This explicitly confirms if the issue is an empty string.\n",
        "\n",
        "            parsed_response = json.loads(json_str)\n",
        "\n",
        "            print(\"\\n--- Structured JSON Response ---\")\n",
        "            print(json.dumps(parsed_response, indent=4))\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå An unexpected error occurred during LLM response generation: {e}\")\n",
        "            print(\"Please inspect the 'Raw LLM Output' above for debugging LLM's raw generation.\")\n",
        "    else:\n",
        "        print(\"\\nGenerative LLM not available. Cannot generate structured answer.\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initializing: Installing necessary libraries ---\n",
            "‚úÖ All required libraries checked/installed successfully.\n",
            "\n",
            "--- Starting Document Processing Pipeline ---\n",
            "\n",
            "--- Tarun's Task: Data Extraction and Cleaning ---\n",
            "Cleaned texts (in JSON) will be saved in: cleaned_texts\n",
            "\n",
            "--- Processing policy.pdf ---\n",
            "Extracting text from local file: policy.pdf\n",
            "‚úÖ Processed policy.pdf\n",
            "\n",
            "üéâ Tarun's Task Completed: All cleaned texts saved to cleaned_texts/all_cleaned_documents.json in JSON format.\n",
            "\n",
            "--- Aahil's Task: Text Chunking and Embedding Generation ---\n",
            "Chunks and embeddings will be saved in: chunks_and_embeddings\n",
            "Loading sentence-transformer model 'all-MiniLM-L6-v2'...\n",
            "‚úÖ Embedding model loaded successfully.\n",
            "Loaded 1 documents for chunking and embedding.\n",
            "Generated 242 chunks from 'policy.pdf'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "741b80129a364d6ea37f3f170665e9b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéâ Aahil's Task Completed: Chunks, embeddings, and source files ready for Bharat.\n",
            "\n",
            "--- Bharat's Task: LLM Document Processing System ---\n",
            "‚úÖ Successfully loaded 242 chunks and embeddings of shape (242, 384)\n",
            "‚úÖ FAISS index created and populated with 242 vectors.\n",
            "Loading query embedding model 'all-MiniLM-L6-v2'...\n",
            "‚úÖ Query embedding model loaded.\n",
            "Loading Generative LLM pipeline: 'google/flan-t5-small'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Generative LLM pipeline loaded successfully on CPU.\n",
            "\n",
            "--- LLM Document Processing System Ready! Type 'exit' to quit. ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2287476680.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- LLM Document Processing System Ready! Type 'exit' to quit. ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0muser_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYour Query: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exiting Document Processing System. Goodbye! üëã\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998,
          "referenced_widgets": [
            "741b80129a364d6ea37f3f170665e9b5",
            "72f3a218d49d4bf18c559e0c056f7aa1",
            "3a7d8285c7394f9bac996bed1c933288",
            "ba18d1a469204f77b05f371f800665cf",
            "c77c469559634004b45d85b586b25918",
            "c4853978089c464da56f7bfcf3ef4b7d",
            "7f78f1e5831940c09b4ea621401dbd37",
            "3c41bf5a10e54dccba939a2b8a041daf",
            "cc46820d72b14918940a3fa8834f8a86",
            "8079967041b64b4ca0467df22972a8fa",
            "c1eb37d50cca4f90b77c5e3ce9b2294b"
          ]
        },
        "id": "flZf0L2ppbjc",
        "outputId": "006eba95-b983-4931-ff28-623910572c7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"\"\"\n",
        "        print(\"\\nGenerating structured response... ü§ñ\")\n",
        "        try:\n",
        "            llm_output = text_generation_pipeline(prompt)[0]['generated_text']\n",
        "            print(\"\\n--- Raw LLM Output (for debugging) ---\")\n",
        "            print(llm_output)\n",
        "\n",
        "            json_match = re.search(r'```json\\n(.*?)```', llm_output, re.DOTALL)\n",
        "            if json_match:\n",
        "                json_str = json_match.group(1).strip()\n",
        "            else:\n",
        "                json_str = llm_output.strip()\n",
        "\n",
        "            parsed_response = json.loads(json_str)\n",
        "\n",
        "            print(\"\\n--- Structured JSON Response ---\")\n",
        "            print(json.dumps(parsed_response, indent=4))\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"‚ùå Error parsing LLM's JSON output: {e}\")\n",
        "            print(\"LLM output was not valid JSON. Please inspect the 'Raw LLM Output' above.\")\n",
        "            print(\"Raw LLM output was:\\n\", llm_output)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå An unexpected error occurred during LLM response generation: {e}\")\n",
        "            print(\"Raw relevant clauses were displayed above.\")\n",
        "    else:\n",
        "        print(\"\\nGenerative LLM not available. Displaying raw relevant clauses instead of a structured answer.\")\n",
        "\n",
        "\n",
        "  ```"
      ],
      "metadata": {
        "id": "UOPdracTpbjg"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "741b80129a364d6ea37f3f170665e9b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72f3a218d49d4bf18c559e0c056f7aa1",
              "IPY_MODEL_3a7d8285c7394f9bac996bed1c933288",
              "IPY_MODEL_ba18d1a469204f77b05f371f800665cf"
            ],
            "layout": "IPY_MODEL_c77c469559634004b45d85b586b25918"
          }
        },
        "72f3a218d49d4bf18c559e0c056f7aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4853978089c464da56f7bfcf3ef4b7d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7f78f1e5831940c09b4ea621401dbd37",
            "value": "Batches:‚Äá100%"
          }
        },
        "3a7d8285c7394f9bac996bed1c933288": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c41bf5a10e54dccba939a2b8a041daf",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc46820d72b14918940a3fa8834f8a86",
            "value": 8
          }
        },
        "ba18d1a469204f77b05f371f800665cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8079967041b64b4ca0467df22972a8fa",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c1eb37d50cca4f90b77c5e3ce9b2294b",
            "value": "‚Äá8/8‚Äá[00:23&lt;00:00,‚Äá‚Äá3.01s/it]"
          }
        },
        "c77c469559634004b45d85b586b25918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4853978089c464da56f7bfcf3ef4b7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f78f1e5831940c09b4ea621401dbd37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c41bf5a10e54dccba939a2b8a041daf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc46820d72b14918940a3fa8834f8a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8079967041b64b4ca0467df22972a8fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1eb37d50cca4f90b77c5e3ce9b2294b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}